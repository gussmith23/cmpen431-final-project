
Foreward / Introduction
Fundamentally, the challenge presented by this design project was one of fuzzy opimization - we were optimizing a simulated architecture across a set of configuration variables whose combined range was simply too large to exhaustively analyze. A brute force approach would imply a test set of roughly 2^9000 configurations. Even if we commandeered the entirety of the 218 Lab computers, we'd fail to make a dent in this expansive set of possibilities in the weeks we had to complete the project. In this paper, we explain both our methodology for automating the process of running selective tests and our methodology for utilizing selective testing to iteratively 'hone-in' on an 'optimum' design.

A Basic Simulation Automation Framework
{A design-doc for the software framework, could be any length, really}


An Approach to De-Fuzzing the Optimization Problem
The process of working towards a psuedo-optimal machine design was two-fold - we attempted to identify 'strong' configuration options, which consistently yielded an overwhelming effect on performance, relative to other options, and 'isolatable' configuration options (or groups of options) which were optimizable (or at least very nearly optimizable) regardless of the other options used.

Identifying strong configuration options began with a look back at previous simplescalar simulation experiments we had run.

Caches and Data Access
The data access mechanics of simplescalar is based on 2 level-one caches (one for each)

The number of data accesses (where 'data' refers to both instructions and data memory being accessed by the program) in a program is contant, and we can view the overall execution time of a program (in cycles, for the sake of simplescalar semantics), as the sum of the time to access the data and the time taken to process that data. The key point here is that these two times can be considered seperately, and, to a point, optimized seperately.

Improvements to instruction memory access times will be almost entirely shared between dynamicly and staticly dispatched processor configurations, since the instruction loads themselves, which account for the vast majority of accesses (since instruction memory is more or less read-only), are executed in order for either style of processor. One caveat is a decrease in available ul2 space






128B >= The ul2 block size >= 2il1 (and  dl1) >= 2(il1 block size (in bytes) = dl1 block size (in bytes) = ifq size (which is in words))


larger ifq would always be better? for either static or dynamic, decode width is the limiting factor, and having a larger ifq than decode width shouldn't help at all. If anything, it might hurt to have a larger queue - on a branch misprediction, we would have to flush more instructions. Also raises the block size of the level 1 caches, which utilizes more temporal locality at the expense of collision misses, which overall should improve local performance at the expense of


latency to main memory isn't ties to the width - which can be 8 or 16 bytes. Might as well make this 16B. 


If the cache is inclusive, the minimum memory access = ul2 block size = 2 il1 size. So, the abs. min = 16B [2x8B]. 

average access time for instuctions = (il1 hit rate *  il1 latency) + 
		(il1 miss rate * ul1 hit rate on intruction attempts (see below)) * (2* il1 latency + ul1 latency) +
		(il1 miss rate * ul1 miss rate on intruction attempts (see below)) * (3 * il1 latency + 2 * ul1 latency)


The data access time equation mirrors this.

In an inclusive cache, a ul1 hit is less likely, since it has a portion of its memory occupied by the identical, hitless memory of the l1 cache. Worse, it has twice this portion, inherently, since an instuction miss can't possibly be found in the inclusive data section reflecting dl1, and vice versa.  So, having a larger ul2 cache, in relation to *l1, is much more important for an inclusive cache (basically, why have a ul2 unless it has a large pool of possible unique blocks?). 


If the cach is strictly exclusive, the minimum memory access = il1 block size. So, the abs. min = 8B.
Skipping the ul2 on a miss would save that access time:
average access time for instuctions = (il1 hit rate *  il1 latency) + 
		(il1 miss rate * ul1 hit rate on intruction attempts (see below)) * (il1 latency + ul1 latency) +
		(il1 miss rate * ul1 miss rate on intruction attempts (see below)) * (2 * il1 latency + ul1 latency)
Again, the data access time equation mirrors this. 
The above assumes that, for an exclusive cache, a memory access that is larger than the block size of the cache it lands in doesn't evict an extra block (specifically, for a 8B l1 cache, a 16B mem access would waste the other 8B, but not put them in l1 in another block).

